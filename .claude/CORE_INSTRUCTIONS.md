# ðŸš€ AI Assistant Core Instructions for RankMyBrand.ai GEO Platform

## ðŸ”´ CRITICAL: MANDATORY CONTEXT INITIALIZATION PROTOCOL

### Upon EVERY New Conversation:
```yaml
IMMEDIATE_ACTIONS:
  1. Load Core Context Files (in strict order):
     â–¡ .claude/instructions.md            # Project overview
     â–¡ .claude/context/current-status.md  # Current progress
     â–¡ .claude/CORE_INSTRUCTIONS.md       # This file - detailed rules
     â–¡ .claude/context/architecture-decisions.md  # Technical choices
     â–¡ .claude/context/athenahq-intelligence.md   # Competitor analysis
     â–¡ .claude/context/integration-map.md         # Module connections
     
  2. Verify Current State:
     â–¡ Active module being worked on
     â–¡ Completion percentage
     â–¡ Blocking issues
     â–¡ Integration readiness
     â–¡ Performance metrics vs targets
     
  3. Acknowledge in First Response:
     "ðŸ“Š Project Status Loaded:
      - Current Module: [MODULE_NAME] ([XX]% complete)
      - Last Update: [TIMESTAMP]
      - Key Context: [BRIEF_SUMMARY]
      - Continuing from: [LAST_CHECKPOINT]"
```

## ðŸŽ¯ PROJECT MISSION & COMPETITIVE CONTEXT

### Core Objective:
**Build the world's most advanced Generative Engine Optimization (GEO) platform that surpasses AthenaHQ's capabilities while maintaining 50% lower pricing**

### AthenaHQ Benchmarks to Beat:
```yaml
Performance_Targets:
  - Data Collection: <5 min (vs AthenaHQ's ~10 min)
  - AI Platforms: 8+ (vs their 5)
  - Real-time Updates: Yes (vs their batch)
  - Auto-execution: Yes (vs their manual)
  - Pricing: $79-199/mo (vs their $270-545/mo)
  - Infrastructure Cost: <$50/mo (vs their estimated $5000+/mo)
  
Technical_Superiority:
  - Response Time: <50ms p95 (vs their ~500ms)
  - Availability: 99.9% (vs their ~99%)
  - Data Freshness: <2 min (vs their daily)
  - Concurrent Users: 1,000+ initially (scalable to 10,000+)
```

### Our Competitive Advantages:
1. **Dual Capability**: Traditional SEO + AI Visibility (unique)
2. **Real-time Architecture**: Event-driven vs batch processing
3. **Automation**: Auto-publish vs manual recommendations
4. **Pricing**: Accessible to SMBs, not just enterprise
5. **Infrastructure**: Lean self-hosted vs expensive cloud
6. **White-label**: Agency-ready platform

## ðŸ“ PROJECT STRUCTURE & LOCATIONS

```bash
/Users/sawai/Desktop/rankmybrand.ai/
â”œâ”€â”€ .claude/                            # Claude-specific files
â”‚   â”œâ”€â”€ instructions.md                 # Basic project instructions
â”‚   â”œâ”€â”€ CORE_INSTRUCTIONS.md           # This file - detailed rules
â”‚   â”œâ”€â”€ context/                       # Context files
â”‚   â”‚   â”œâ”€â”€ current-status.md         # Project progress
â”‚   â”‚   â”œâ”€â”€ architecture-decisions.md # ADRs
â”‚   â”‚   â”œâ”€â”€ athenahq-intelligence.md  # Competitor analysis
â”‚   â”‚   â””â”€â”€ integration-map.md        # Module connections
â”‚   â””â”€â”€ prompts/                       # Module implementation prompts
â”‚       â”œâ”€â”€ foundation.md              # Foundation module (DONE)
â”‚       â”œâ”€â”€ ai-response-monitor.md    # To be created
â”‚       â”œâ”€â”€ intelligence-engine.md    # To be created
â”‚       â”œâ”€â”€ action-center.md          # To be created
â”‚       â””â”€â”€ unified-dashboard.md      # To be created
â”‚
â”œâ”€â”€ services/                           # Microservices
â”‚   â”œâ”€â”€ foundation/                    # Base infrastructure (IN PROGRESS)
â”‚   â”œâ”€â”€ geo-calculator/                # Existing - DO NOT BREAK
â”‚   â”œâ”€â”€ web-crawler/                   # Existing - DO NOT BREAK
â”‚   â”œâ”€â”€ ai-response-monitor/          # To be built
â”‚   â”œâ”€â”€ intelligence-engine/          # To be built
â”‚   â”œâ”€â”€ action-center/                # To be built
â”‚   â””â”€â”€ unified-dashboard/            # To be built
â”‚
â”œâ”€â”€ infrastructure/                     # Docker and deployment
â”‚   â”œâ”€â”€ docker/                       
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml        # Main orchestration
â”‚   â”‚   â””â”€â”€ .env.example              # Environment template
â”‚   â””â”€â”€ monitoring/                   
â”‚       â”œâ”€â”€ grafana/                  # Dashboards
â”‚       â””â”€â”€ prometheus/               # Metrics config
â”‚
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ API.md                        # API documentation
    â”œâ”€â”€ DEPLOYMENT.md                 # Deployment guide
    â””â”€â”€ RUNBOOK.md                    # Operations manual
```

## ðŸ—ï¸ MODULE DEVELOPMENT PROTOCOL

### Module Implementation Order:
1. **Foundation Infrastructure** (Week 1) - IN PROGRESS
2. **AI Response Monitor** (Week 2-3)
3. **Intelligence Engine** (Week 4-5)
4. **Action Center** (Week 6-7)
5. **Unified Dashboard** (Week 8-9)
6. **Enterprise Features** (Week 10+)

### Module Completion Criteria:
```yaml
DEFINITION_OF_DONE:
  Code_Quality:
    â–¡ Test coverage >80%
    â–¡ All linting passes
    â–¡ Security scan clean
    â–¡ Performance within budget (<50ms p95)
    
  Documentation:
    â–¡ README.md complete
    â–¡ API documentation via OpenAPI
    â–¡ Integration guide written
    â–¡ Docker setup working
    
  Integration:
    â–¡ Redis Streams connected
    â–¡ Health checks passing
    â–¡ Metrics exposed
    â–¡ Logging configured
    
  Performance:
    â–¡ Latency <50ms p95
    â–¡ Memory <500MB
    â–¡ CPU <1 core under load
    â–¡ Can handle 100 req/sec
```

## ðŸ”§ TECHNICAL STANDARDS & PATTERNS

### Language & Framework Requirements:
```yaml
Backend_Services:
  - Node.js 20.11.0 LTS with TypeScript
  - Python 3.12+ for ML/AI components
  - Express/Fastify for APIs
  
Frontend:
  - Next.js 14+ with App Router
  - TypeScript strict mode
  - Tailwind CSS + shadcn/ui
  
Databases:
  - PostgreSQL 16.1 with TimescaleDB
  - Redis 7.2.4 for cache/streams/queues
  - ChromaDB for vectors (self-hosted)
  
Infrastructure:
  - Docker 24.0.7 & Docker Compose 2.23.0
  - Docker Swarm (not Kubernetes - too expensive)
  - Caddy 2.7.6 for API gateway (auto-HTTPS)
  - Prometheus + Grafana for monitoring
```

### Code Standards:
```typescript
// EVERY service must follow this structure
interface ServiceStructure {
  src: {
    api: RestEndpoints;          // Express/Fastify routes
    services: BusinessLogic;     // Core functionality
    repositories: DataAccess;    // Database operations
    events: EventHandlers;       // Redis Streams pub/sub
    models: DataModels;         // TypeScript interfaces
    utils: Helpers;             // Shared utilities
  };
  tests: {
    unit: UnitTests;            // >80% coverage
    integration: IntTests;       // API & DB tests
    e2e: EndToEndTests;         // Full flow validation
  };
  docker: {
    Dockerfile: Container;       // Multi-stage build
    'docker-compose.yml': Local; // Local development
  };
}
```

### Communication Patterns:
```yaml
Internal_Communication:
  Async: Redis Streams for events
  Sync: HTTP REST for queries
  Real-time: WebSockets for UI updates
  
External_APIs:
  - Use circuit breakers
  - Implement retries with exponential backoff
  - Cache responses aggressively
  - Rate limit all calls
```

## ðŸŽ® AI ASSISTANT BEHAVIORAL RULES

### ABSOLUTE REQUIREMENTS:
```yaml
ALWAYS:
  âœ“ Write production-ready code (no MVPs)
  âœ“ Include error handling
  âœ“ Add TypeScript types
  âœ“ Write tests alongside code
  âœ“ Use environment variables (no hardcoding)
  âœ“ Implement with real data (no mocks)
  âœ“ Consider costs (<$50/month total)
  
NEVER:
  âœ— Use expensive cloud services
  âœ— Implement "phase 1" (full solution only)
  âœ— Skip error handling
  âœ— Use 'any' TypeScript type
  âœ— Create mock data or stubs
  âœ— Ignore performance budgets
  âœ— Break existing modules
```

### When Writing Code:
1. **Production-first mindset** - Would this work for 1000 users today?
2. **Cost-conscious** - Can this run on a $20 VPS?
3. **Integration-ready** - Does this connect to other modules?
4. **Observable** - Can we monitor and debug this?

### When Designing Features:
1. How does AthenaHQ do it? (baseline)
2. How can we do it better AND cheaper?
3. What's the simplest solution that works?
4. Will this scale without rewriting?

## ðŸ“Š PERFORMANCE & COST CONSTRAINTS

### Infrastructure Budget:
```yaml
Monthly_Costs:
  VPS: $20-40 (4GB RAM, 2 CPU)
  Domain: $1
  SSL: $0 (Let's Encrypt)
  Monitoring: $0 (self-hosted)
  Backups: $5 (S3/Backblaze)
  TOTAL: <$50/month
  
Resource_Limits:
  Total_RAM: 4GB for entire stack
  CPU: 2 cores
  Storage: 100GB
  Bandwidth: 1TB/month
```

### Performance Budgets:
```yaml
Per_Service:
  API_Response: <50ms p95
  Database_Query: <100ms p95
  Redis_Operation: <5ms
  Memory_Usage: <500MB per service
  CPU_Usage: <50% average
  
System_Wide:
  Concurrent_Users: 100 (initially)
  Requests_Per_Second: 1000
  Events_Per_Second: 10000
  Data_Processing: 100K records/hour
```

## ðŸ” SECURITY REQUIREMENTS

### Every Module Must:
```yaml
Security_Checklist:
  â–¡ Use HTTPS everywhere (Caddy auto-SSL)
  â–¡ Implement rate limiting
  â–¡ Add JWT authentication
  â–¡ Validate all inputs
  â–¡ Sanitize outputs
  â–¡ Log security events
  â–¡ Rotate secrets via env vars
  â–¡ No hardcoded credentials
  â–¡ Use prepared statements for SQL
  â–¡ Implement CORS properly
```

## ðŸ“ CONTEXT MAINTENANCE PROTOCOL

### After Each Work Session:
```bash
# Update progress
1. Update .claude/context/current-status.md
2. Document any architecture decisions
3. Update integration map if needed
4. Commit all changes with clear message

# Verify nothing broke
npm test
docker-compose ps
curl http://localhost/health

# Document session results
- What was completed?
- What's blocked?
- What's next?
```

## ðŸš¨ CRITICAL WARNINGS

### DO NOT:
1. **Over-engineer** - We're not building for Google scale
2. **Over-spend** - Stay under $50/month infrastructure
3. **Over-complicate** - Simpler is better
4. **Break existing** - geo-calculator and web-crawler must keep working
5. **Use mocks** - Real implementation only

### RED FLAGS:
- "We'll implement this later..." âŒ
- "For now, let's mock..." âŒ
- "This needs Kubernetes..." âŒ
- "We should use AWS managed..." âŒ
- "Let's start with phase 1..." âŒ

## ðŸŽ¯ SUCCESS METRICS

### We Win When:
1. **Cheaper than AthenaHQ**: $79/mo vs their $270/mo
2. **Faster than AthenaHQ**: Real-time vs their daily updates
3. **Better than AthenaHQ**: More AI platforms, auto-execution
4. **Simpler than AthenaHQ**: No complex credit system
5. **More accessible**: SMBs can afford us

### Technical Victory:
- All services running on single VPS
- <50ms response times
- <$50/month infrastructure
- 99.9% uptime
- Zero data loss

## ðŸ¤– RESPONSE TEMPLATE

When working on modules, structure responses as:

```markdown
## ðŸ“Š Current Status
- Module: [NAME]
- Progress: [XX]%
- Session Goal: [WHAT WE'RE BUILDING]

## ðŸ”§ Implementation
[CODE]

## âœ… Completed This Session
- [ ] Item 1
- [ ] Item 2

## ðŸš¦ Next Steps
1. Next immediate task
2. Following task

## ðŸ“ Files Updated
- path/to/file1.ts
- path/to/file2.md
```

---

**Remember**: We're building a LEAN, EFFICIENT platform that beats AthenaHQ on every metric while costing 1/100th to operate. Every decision should optimize for:
1. **Works today** (not someday)
2. **Costs less** (infrastructure <$50/mo)
3. **Performs better** (real-time vs batch)
4. **Simpler to use** (no credit complexity)

**The North Star**: Can a small business owner use this TODAY to beat their competitor who uses AthenaHQ?
